{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Web Scraping Review! \n", "\n", "<img src=https://media.giphy.com/media/U4FkC2VqpeNRHjTDQ5/giphy.gif width=\"500px\"></img>\n", "\n", "\n", "## There's been a lot of talk about web scraping lately. \n", "So let's work through a web scraping problem as a refresher. \n", "\n", "    \n", "### Goals:\n", "1. Retrieve the HTML of a webpage with the `requests` library.\n", "2. Introduction to the `tree` structure of `HTML`.\n", "3. Use the `inspect` tool to sift through the HTML.\n", "4. Parse HTML with the `BeautifulSoup` library.\n", "5. Store data in a `csv` file using the `Pandas` library.\n", "\n", "# Let's scrape some data!\n", "The data we are scraping today will be from the [Quotes to Scrape](http://quotes.toscrape.com/) website.\n", "\n", "\n", "## Step 1:\n", "> **Import the necessary tools for our project**\n", "\n", "![](https://media.giphy.com/media/KcE7Dq5f8TTXzZ1LAF/giphy.gif)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-19T22:34:28.023498Z", "start_time": "2020-07-19T22:34:27.405317Z"}}, "outputs": [], "source": ["# Webscraping\n", "import requests\n", "from bs4 import BeautifulSoup\n", "\n", "# Data organization\n", "import pandas as pd\n", "\n", "# Visualization\n", "import matplotlib.pyplot as plt\n", "plt.rcParams.update({'font.size': 22})\n", "\n", "from test_scripts.test_class import Test\n", "test = Test()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2\n", "> **We use the `requests` library to connect to the website we wish to scrape.**\n", "\n", "<img src='https://media.giphy.com/media/eCwAEs05phtK/giphy.gif' width='300'></img>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:44:52.105946Z", "start_time": "2020-07-10T16:44:51.803036Z"}}, "outputs": [], "source": ["url = 'http://quotes.toscrape.com'\n", "response = requests.get(url)\n", "response"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u2705**A `Response 200` means our request was sucessful!** \u2705\n", "\n", "\u274cLet's take a quick look at an *unsuccessful* response. \u274c"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:45:00.840806Z", "start_time": "2020-07-10T16:45:00.571721Z"}}, "outputs": [], "source": ["bad_url = 'http://quotes.toscrape.com/20'\n", "requests.get(bad_url)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**A `Response 404` means that the url you are using in your request is not pointing to a valid webpage.**\n", "\n", "<img src='https://media.giphy.com/media/VwoJkTfZAUBSU/giphy.gif' width='300'></img>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3\n", "> **We collect the html from the website by adding `.text` to the end of the response variable.** "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:46:24.428053Z", "start_time": "2020-07-10T16:46:24.423335Z"}}, "outputs": [], "source": ["html = response.text\n", "html[:50]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4\n", "> **We use `BeautifulSoup` to turn the html into something we can manipulate.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:47:31.277945Z", "start_time": "2020-07-10T16:47:31.236323Z"}}, "outputs": [], "source": ["soup = BeautifulSoup(html, 'lxml')\n", "soup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><h1><b><i><u>Very Soupy</u></i></b></h1></center>\n", "\n", "The name ***Beautiful Soup*** is an appropriate description. HTML does not make for a lovely reading experience. If you feel like you're staring at complete gibberish, you're not entirely wrong! HTML is a language designed for computers, not for human eyes.\n", "\n", "<img src='https://media.giphy.com/media/5xtDarBbqdSQxfGFdNS/giphy.gif' width=\"200\"></img>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Fortunately for us,** <u>we do not have to read through every line of the html in order to web scrape.</u> \n", "\n", "Modern day web browsers come equipped with tools that allow us to easily sift through this soupy text.\n", "\n", "\n", "## Step 4\n", ">**We open up the page we're trying to scrape in a new tab.** <b><a href='http://quotes.toscrape.com/' target='_blank'>Click Here!</a></b> \ud83d\udc40\n", "\n", "Once we've opened up the webpage, we use the `inspect` tool to locate the html for the elements we wish to scrape.\n", "\n", "![](static/inspect-gif.gif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5\n", "   > **We create a list of every `div` that has a `class` of \"quote\"**\n", "\n", "**In this instance,** every item we want to collect is divided into identically labeled containers.\n", "- A div with a class of 'quote'.\n", "\n", "Not all HTML is as well organized as this page, but HTML is basically just a bunch of different organizational containers that we use to divide up text and other forms of media. Figuring out the organizational structure of a website is the entire process for web scraping. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:52:54.369535Z", "start_time": "2020-07-10T16:52:54.363659Z"}}, "outputs": [], "source": ["quote_divs = soup.find_all('div', {'class': 'quote'})\n", "len(quote_divs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 6\n", "> **To figure out how to grab all the datapoints from a quote, we isolate a single quote, and work through the code for a single `div`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:55:03.971169Z", "start_time": "2020-07-10T16:55:03.968308Z"}}, "outputs": [], "source": ["first_quote = quote_divs[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### First we grab the text of the quote"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:55:09.147053Z", "start_time": "2020-07-10T16:55:09.141928Z"}}, "outputs": [], "source": ["text = first_quote.find('span', {'class':'text'})\n", "text = text.text\n", "text"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Next we grab the author"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T16:56:18.149727Z", "start_time": "2020-07-10T16:56:18.144204Z"}}, "outputs": [], "source": ["author = first_quote.find('small', {'class': 'author'})\n", "author_name = author.text\n", "author_name"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's also grab the link that points to the author's bio!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:00:04.320989Z", "start_time": "2020-07-10T17:00:04.316244Z"}}, "outputs": [], "source": ["author_link = author.findNextSibling().attrs.get('href')\n", "author_link = url + author_link\n", "author_link"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### And finally, let's grab all of the tags for the quote"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:01:57.978855Z", "start_time": "2020-07-10T17:01:57.973531Z"}}, "outputs": [], "source": ["tag_container = first_quote.find('div', {'class': 'tags'})\n", "tag_links = tag_container.find_all('a')\n", "\n", "tags = []\n", "for tag in tag_links:\n", "    tags.append(tag.text)\n", "    \n", "tags"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Our data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:02:36.746672Z", "start_time": "2020-07-10T17:02:36.743338Z"}}, "outputs": [], "source": ["print('text:', text)\n", "print('author name: ', author_name)\n", "print('author link: ', author_link)\n", "print('tags: ', tags)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 7\n", "> We create a function to make our code reusable.\n", "\n", "Now that we know how to collect this data from a quote div, we can compile the code into a [function](https://www.geeksforgeeks.org/functions-in-python/) called `quote_data`. This allows us to grab a quote div, feed it into the function like so...\n", "> `quote_data(quote_div)`\n", "\n", "...and receive all of the data from that div.\n", "\n", "<u><b>In the cell below, define a function called `quote_data` that receives a single quote_div and does the following</b></u>\n", "1. Collects the text from the quote\n", "2. Collects the author name\n", "3. Collects the author link\n", "4. Collects the list of tags assigned to the quote. \n", "5. Returns the data in the following format\n", "```\n", "{'author': author_name,\n", " 'text': text,\n", " 'author_link': author_link,\n", " 'tags': tags}\n", " ```"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "def quote_data(quote_div):\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's test our fuction."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell unchanged\n", "test.run_test(quote_data(quote_divs[7]), 'quote_scraper')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can collect the data from every quote on the first page with a simple [```for loop```](https://www.w3schools.com/python/python_for_loops.asp)!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["page_one_data = []\n", "for div in quote_divs:\n", "    # Apply our function to each quote\n", "    data_from_div = quote_data(div)\n", "    page_one_data.append(data_from_div)\n", "    \n", "print(len(page_one_data), 'quotes scraped!')\n", "page_one_data[:2]"]}, {"cell_type": "markdown", "metadata": {"ExecuteTime": {"end_time": "2020-07-03T18:55:14.762708Z", "start_time": "2020-07-03T18:55:14.758667Z"}}, "source": ["# We just scraped an entire webpage!\n", "\n", "![](https://media.giphy.com/media/KiXl0vfc9XIIM/giphy.gif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Level Up: What if we wanted to scrape the quotes from *every* page?\n", "\n", "**Step 1:** The first thing we do is take the code from above that scraped the data for all of the quotes on page one, and move it into a function called `parse_page`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:10:40.926551Z", "start_time": "2020-07-10T17:10:40.923140Z"}}, "outputs": [], "source": ["def scrape_page(quote_divs):\n", "    data = []\n", "    for div in quote_divs:\n", "        div_data = quote_data(div)\n", "        data.append(div_data)\n", "        \n", "    return data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 2:** We grab the code we used at the very beginning to collect the html and the list of divs from a web page."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:11:48.656606Z", "start_time": "2020-07-10T17:11:48.366931Z"}}, "outputs": [], "source": ["base_url = 'http://quotes.toscrape.com'\n", "response = requests.get(url)\n", "html = response.text\n", "soup = BeautifulSoup(html, 'lxml')\n", "quote_divs = soup.find_all('div', {'class': 'quote'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 3:** We feed all of the `quote_divs` into our newly made `parse_page` function to grab all of the data from that page."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:11:52.911796Z", "start_time": "2020-07-10T17:11:52.907812Z"}}, "outputs": [], "source": ["data = scrape_page(quote_divs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:11:59.322162Z", "start_time": "2020-07-10T17:11:59.317619Z"}}, "outputs": [], "source": ["data[:2]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 4:** We check to see if there is a `Next Page` button at the bottom of the page.\n", "\n", "*This is requires multiple steps.*\n", "\n", "1. We grab the outer container called that has a class of `pager`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:13:24.139244Z", "start_time": "2020-07-10T17:13:24.135275Z"}}, "outputs": [], "source": ["pager = soup.find('ul', {'class': 'pager'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If there is no pager element on the webpage, pager will be set to `None`.\n", "\n", "2. We use an if check to make sure a pager exists:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:15:25.071467Z", "start_time": "2020-07-10T17:15:25.068029Z"}}, "outputs": [], "source": ["if pager:\n", "    next_page = pager.find('li', {'class': 'next'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3. We then check to see if a `Next Page` button exists on the page. \n", "\n", "    - Every page has a next button except the *last* page which only has a `Previous Page` button. Basically, we're checking to see if the `Next button` exists. It it does, we \"click\" it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:15:25.640201Z", "start_time": "2020-07-10T17:15:25.637151Z"}}, "outputs": [], "source": ["if next_page:\n", "    next_page = next_page.findChild('a')\\\n", "                         .attrs\\\n", "                         .get('href')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With most webscraping tools, \"clicking a button\" means collecting the link inside the button and making a new request.\n", "\n", "If a link is pointing to a page on the same website, the links are usually just the forward slashes that need to be added to the base website url. This is called a `relative` link.\n", "\n", "**Step 5:** Collect the relative link that points to the next page, and add it to our base_url"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:15:39.764653Z", "start_time": "2020-07-10T17:15:39.760665Z"}}, "outputs": [], "source": ["next_page = url + next_page\n", "next_page"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 6:** We repeat the exact same process for this new link!\n", "\n", "ie:\n", "1. Make request using a url that points to the next page.\n", "2. Scrape quote divs\n", "3. Collect data from every quote div on that page\n", "4. Find the `Next page` button.\n", "5. Collect the url from the button\n", "6. Repeat\n", "\n", "**So how do we do this over and over again without repeating ourselves?**\n", "\n", "The first step is compile all of these steps into a new function called `scrape_quotes`.\n", "\n", "The second step is, something called `recursion`. \n", "\n", "<center><h1><u>Recursion</u></h1></center>\n", "\n", "![](https://media.giphy.com/media/l1J9R1Q7LJGSZOxFe/giphy.gif)\n", "\n", "> **Recursion** is a bit of a mind bend, so don't feel bad if it is hard to wrap your brain around. It took me a while to be able to understand recursive functions!\n", "\n", "Essentially, recursion is where we use a function inside of itself.\n", "\n", "**In this instance,** our code will be telling the computer, if there is a `Next page` button, rerun all of the code again on the page the next button points us to and check to see if there is a `Next page` button on *that* page. If there is, keep repeating the process until a `Next page` button is not found."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:19:58.694086Z", "start_time": "2020-07-10T17:19:58.613897Z"}}, "outputs": [], "source": ["def scrape_quotes(url):\n", "    base_url = 'http://quotes.toscrape.com'\n", "    response = requests.get(url)\n", "    html = response.text\n", "    soup = BeautifulSoup(html, 'lxml')\n", "    quote_divs = soup.find_all('div', {'class': 'quote'})\n", "    data = scrape_page(quote_divs)\n", "    \n", "    pager = soup.find('ul', {'class': 'pager'})\n", "    if pager:\n", "        next_page = pager.find('li', {'class': 'next'})\n", "        \n", "        if next_page:\n", "            next_page =  next_page.findChild('a')\\\n", "                                  .attrs\\\n", "                                  .get('href')\n", "            \n", "            next_page = base_url + next_page\n", "            print('Scraping', next_page)\n", "            ## This is where the recursion happens\n", "            data += scrape_quotes(next_page)\n", "        \n", "    \n", "\n", "    return data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can set a variable called `data` that is the output of our recursive function!\n", "\n", "> A print statement has been added to output what page is being scraped"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:20:37.075053Z", "start_time": "2020-07-10T17:20:34.359779Z"}, "scrolled": true}, "outputs": [], "source": ["data = scrape_quotes(url)\n", "print(len(data), 'Quotes scraped!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can visualize and explore our data!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:20:51.008980Z", "start_time": "2020-07-10T17:20:50.800018Z"}}, "outputs": [], "source": ["def count_tags(quote):\n", "    return len(quote['tags'])\n", "\n", "def tag_lengths(data):\n", "    lengths = []\n", "    for quote in data:\n", "        lengths.append(count_tags(quote))\n", "        \n", "    return lengths\n", "        \n", "lengths = tag_lengths(data)\n", "plt.figure(figsize=(10,6))   \n", "plt.hist(lengths, bins=9)\n", "plt.title('Number of Tags');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Saving our data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:21:30.144815Z", "start_time": "2020-07-10T17:21:30.116215Z"}}, "outputs": [], "source": ["df = pd.DataFrame(data)\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:22:10.108560Z", "start_time": "2020-07-10T17:22:10.097506Z"}}, "outputs": [], "source": ["df.to_csv('quotes_data.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Web scraping is a powerful tool. \n", "\n", "It can be used:\n", "- To discover the most in demand skills from thousands of online job postings.\n", "- To learn the average price of a product from thousands of online sale.\n", "- To research social media networks.\n", "\n", "**And so much more!** As our world continues to develop online markets and communities, the uses for webscraping continue to grow. In the end, the power of web scraping comes from the ability to create datasets that otherwise do not exist, or at the very least, are not readily available to the public.\n"]}], "metadata": {"hide_input": false, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {"height": "calc(100% - 180px)", "left": "10px", "top": "150px", "width": "303.797px"}, "toc_section_display": true, "toc_window_display": true}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 2}